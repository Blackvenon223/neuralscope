"""Tests for LLM-based vulnerability scanning."""

import json

from neuralscope.features.vulnerability_scan.data.datasource.llm_scanner.implementation import (
    LlmSecurityScanner,
)
from neuralscope.features.vulnerability_scan.domain.entities.vulnerability import (
    Vulnerability,
    VulnReport,
    VulnSeverity,
    VulnSource,
)


def test_vuln_report_counters():
    report = VulnReport(
        project_path="/project",
        vulnerabilities=[
            Vulnerability(
                id="SEC-1", title="assert",
                severity=VulnSeverity.LOW, source=VulnSource.AI,
            ),
            Vulnerability(
                id="SEC-2", title="sqli",
                severity=VulnSeverity.HIGH, source=VulnSource.AI,
            ),
            Vulnerability(
                id="SEC-3", title="rce",
                severity=VulnSeverity.CRITICAL, source=VulnSource.AI,
            ),
        ],
    )
    assert report.total == 3
    assert report.critical_count == 1
    assert report.high_count == 1
    assert not report.passed


def test_vuln_report_passed():
    report = VulnReport(
        project_path="/project",
        vulnerabilities=[
            Vulnerability(
                id="SEC-1", title="info",
                severity=VulnSeverity.LOW, source=VulnSource.AI,
            ),
        ],
    )
    assert report.passed


def test_llm_scanner_parse_valid():
    raw = json.dumps({
        "vulnerabilities": [
            {
                "id": "SEC-001",
                "title": "SQL Injection",
                "severity": "high",
                "line": 42,
                "description": "User input in raw SQL",
                "recommendation": "Use parameterized queries",
                "cwe": "89",
            },
            {
                "id": "SEC-002",
                "title": "Hardcoded secret",
                "severity": "critical",
                "line": 10,
                "description": "API key in source",
                "recommendation": "Use env vars",
                "cwe": "798",
            },
        ]
    })

    scanner = LlmSecurityScanner.__new__(LlmSecurityScanner)
    vulns = scanner._parse("app.py", raw)
    assert len(vulns) == 2
    assert vulns[0].severity == VulnSeverity.HIGH
    assert vulns[0].cwe == "89"
    assert vulns[0].source == VulnSource.AI
    assert vulns[1].severity == VulnSeverity.CRITICAL


def test_llm_scanner_parse_empty():
    scanner = LlmSecurityScanner.__new__(LlmSecurityScanner)
    vulns = scanner._parse("safe.py", '{"vulnerabilities": []}')
    assert vulns == []


def test_llm_scanner_parse_malformed():
    scanner = LlmSecurityScanner.__new__(LlmSecurityScanner)
    vulns = scanner._parse("test.py", "not json at all")
    assert vulns == []


def test_llm_scanner_extract_json_fences():
    text = '```json\n{"vulnerabilities": []}\n```'
    assert LlmSecurityScanner._extract_json(text) == '{"vulnerabilities": []}'

"""LLM-based security vulnerability scanner."""

from __future__ import annotations

import json
import re
from typing import Any

from langchain_core.language_models import BaseChatModel
from langchain_core.messages import HumanMessage, SystemMessage

from neuralscope.core.logging import get_logger
from neuralscope.features.vulnerability_scan.domain.entities.vulnerability import (
    Vulnerability,
    VulnSeverity,
    VulnSource,
)

logger = get_logger("llm_scanner")

SYSTEM_PROMPT = """\
You are an expert security auditor. Analyze the provided Python source code for vulnerabilities.

Return a JSON object:
{
  "vulnerabilities": [
    {
      "id": "<CWE-ID or custom ID like SEC-001>",
      "title": "<short title>",
      "severity": "low|medium|high|critical",
      "line": <int>,
      "description": "<what's dangerous and why>",
      "recommendation": "<how to fix>",
      "cwe": "<CWE number if applicable>"
    }
  ]
}

Focus on real security issues:
- SQL injection, XSS, command injection
- Hardcoded secrets, weak crypto
- Path traversal, insecure deserialization
- Unsafe eval/exec, unvalidated input
- SSRF, open redirects, auth bypass

Do NOT flag style or convention issues. Return ONLY the JSON object."""

SEVERITY_MAP = {
    "low": VulnSeverity.LOW,
    "medium": VulnSeverity.MEDIUM,
    "high": VulnSeverity.HIGH,
    "critical": VulnSeverity.CRITICAL,
}


class LlmSecurityScanner:
    def __init__(self, llm: BaseChatModel) -> None:
        self._llm = llm

    async def scan_source(self, file_path: str, source: str) -> list[Vulnerability]:
        prompt = f"File: {file_path}\n\n```python\n{source}\n```"
        response = await self._llm.ainvoke(
            [
                SystemMessage(content=SYSTEM_PROMPT),
                HumanMessage(content=prompt),
            ]
        )
        return self._parse(file_path, str(response.content))

    def _parse(self, file_path: str, raw: str) -> list[Vulnerability]:
        try:
            cleaned = self._extract_json(raw)
            data: dict[str, Any] = json.loads(cleaned)
        except (json.JSONDecodeError, ValueError):
            logger.warning("Failed to parse LLM scanner response for %s", file_path)
            return []

        vulns = []
        for v in data.get("vulnerabilities", []):
            vulns.append(
                Vulnerability(
                    id=v.get("id", ""),
                    title=v.get("title", ""),
                    severity=SEVERITY_MAP.get(v.get("severity", "low"), VulnSeverity.LOW),
                    source=VulnSource.AI,
                    file_path=file_path,
                    line=v.get("line", 0),
                    description=v.get("description", ""),
                    recommendation=v.get("recommendation", ""),
                    cwe=str(v.get("cwe", "")),
                )
            )
        return vulns

    @staticmethod
    def _extract_json(text: str) -> str:
        match = re.search(r"```(?:json)?\s*(\{.*\})\s*```", text, re.DOTALL)
        if match:
            return match.group(1)
        start = text.find("{")
        end = text.rfind("}") + 1
        if start >= 0 and end > start:
            return text[start:end]
        return text
